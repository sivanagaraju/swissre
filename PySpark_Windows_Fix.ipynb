{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PySpark Windows Environment Diagnostic & Fix\n",
                "\n",
                "This notebook will help identify and fix Windows-specific PySpark issues."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Check Current Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "\n",
                "print(\"Python Version:\", sys.version)\n",
                "print(\"Python Executable:\", sys.executable)\n",
                "\n",
                "# Check PySpark version\n",
                "try:\n",
                "    import pyspark\n",
                "    print(\"PySpark Version:\", pyspark.__version__)\n",
                "except Exception as e:\n",
                "    print(\"PySpark Error:\", e)\n",
                "\n",
                "# Check PyArrow (often the culprit on Windows)\n",
                "try:\n",
                "    import pyarrow\n",
                "    print(\"PyArrow Version:\", pyarrow.__version__)\n",
                "except:\n",
                "    print(\"⚠️ PyArrow NOT installed\")\n",
                "\n",
                "# Check Java\n",
                "import subprocess\n",
                "try:\n",
                "    result = subprocess.run([\"java\", \"-version\"], capture_output=True, text=True)\n",
                "    print(\"\\nJava Version:\")\n",
                "    print(result.stderr.split('\\n')[0])\n",
                "except:\n",
                "    print(\"⚠️ Java NOT found in PATH\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Install Required Packages\n",
                "\n",
                "PyArrow is critical for Windows PySpark compatibility."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install pyarrow if missing\n",
                "!pip install pyarrow --upgrade"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: Configure Spark for Windows\n",
                "\n",
                "Set critical environment variables before creating Spark session."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Critical: Set Python executables\n",
                "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
                "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
                "\n",
                "# Disable PyArrow optimization that can cause issues\n",
                "os.environ['PYARROW_IGNORE_TIMEZONE'] = '1'\n",
                "\n",
                "print(\"✅ Environment configured\")\n",
                "print(f\"PYSPARK_PYTHON: {os.environ['PYSPARK_PYTHON']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Create Spark Session with Windows-Optimized Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Create Spark session with Windows-friendly settings\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"WindowsTest\") \\\n",
                "    .master(\"local[1]\") \\\n",
                "    .config(\"spark.driver.memory\", \"2g\") \\\n",
                "    .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
                "    .config(\"spark.ui.enabled\", \"false\") \\\n",
                "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
                "    .config(\"spark.python.worker.reuse\", \"false\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"✅ Spark Session Created\")\n",
                "print(f\"Spark Version: {spark.version}\")\n",
                "print(f\"Spark Master: {spark.sparkContext.master}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 5: Test Basic Operations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import Row\n",
                "\n",
                "# Test 1: Simple data creation\n",
                "print(\"Test 1: Creating simple DataFrame...\")\n",
                "try:\n",
                "    simple_data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")]\n",
                "    df = spark.createDataFrame(simple_data, [\"id\", \"name\"])\n",
                "    print(\"✅ DataFrame created successfully\")\n",
                "    \n",
                "    # Test show()\n",
                "    df.show()\n",
                "    print(\"✅ show() works!\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test 2: Row-based creation (what the ETL uses)\n",
                "print(\"\\nTest 2: Creating DataFrame with Row objects...\")\n",
                "try:\n",
                "    from datetime import date\n",
                "    \n",
                "    row_data = [\n",
                "        Row(id=1, name=\"Alice\", date=date(2024, 1, 1)),\n",
                "        Row(id=2, name=\"Bob\", date=date(2024, 1, 2)),\n",
                "    ]\n",
                "    df2 = spark.createDataFrame(row_data)\n",
                "    print(\"✅ Row-based DataFrame created\")\n",
                "    \n",
                "    df2.show()\n",
                "    print(\"✅ Row-based show() works!\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test 3: Operations\n",
                "print(\"\\nTest 3: Testing transformations...\")\n",
                "try:\n",
                "    from pyspark.sql.functions import col, upper\n",
                "    \n",
                "    df3 = df.withColumn(\"upper_name\", upper(col(\"name\")))\n",
                "    df3.show()\n",
                "    print(\"✅ Transformations work!\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Error: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test 4: Simple UDF\n",
                "print(\"\\nTest 4: Testing simple UDF...\")\n",
                "try:\n",
                "    from pyspark.sql.functions import udf\n",
                "    from pyspark.sql.types import StringType\n",
                "    \n",
                "    def add_prefix(name):\n",
                "        return f\"Mr. {name}\"\n",
                "    \n",
                "    add_prefix_udf = udf(add_prefix, StringType())\n",
                "    \n",
                "    df4 = df.withColumn(\"prefixed\", add_prefix_udf(col(\"name\")))\n",
                "    df4.show()\n",
                "    print(\"✅ UDF works!\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ UDF Error: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 6: Write Test (Critical for ETL)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tempfile\n",
                "import os\n",
                "\n",
                "print(\"Test 5: Testing write operations...\")\n",
                "\n",
                "# Test CSV write\n",
                "try:\n",
                "    temp_dir = tempfile.mkdtemp()\n",
                "    csv_path = os.path.join(temp_dir, \"test_csv\")\n",
                "    \n",
                "    df.coalesce(1).write.csv(csv_path, mode=\"overwrite\", header=True)\n",
                "    print(\"✅ CSV write works!\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ CSV write failed: {e}\")\n",
                "\n",
                "# Test Parquet write\n",
                "try:\n",
                "    parquet_path = os.path.join(temp_dir, \"test_parquet\")\n",
                "    \n",
                "    df.coalesce(1).write.parquet(parquet_path, mode=\"overwrite\")\n",
                "    print(\"✅ Parquet write works!\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Parquet write failed: {e}\")\n",
                "\n",
                "# Test pandas conversion (workaround)\n",
                "try:\n",
                "    import pandas as pd\n",
                "    \n",
                "    pandas_df = df.toPandas()\n",
                "    csv_file = os.path.join(temp_dir, \"pandas_test.csv\")\n",
                "    pandas_df.to_csv(csv_file, index=False)\n",
                "    print(\"✅ Pandas conversion and write works!\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"❌ Pandas approach failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary & Recommendations\n",
                "\n",
                "Based on which tests passed/failed above:\n",
                "\n",
                "### If All Tests Pass:\n",
                "✅ Your environment is fixed! The ETL notebook should now work.\n",
                "\n",
                "### If show() Fails:\n",
                "- Reinstall pyarrow: `pip uninstall pyarrow && pip install pyarrow`\n",
                "- Check Java version (needs Java 11 or 17)\n",
                "\n",
                "### If UDF Fails:\n",
                "- Use `local[1]` instead of `local[*]`\n",
                "- Set `spark.python.worker.reuse` to `false`\n",
                "\n",
                "### If Write Fails:\n",
                "- Use pandas workaround: `df.toPandas().to_csv()`\n",
                "- This is what we implemented in the local notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cleanup\n",
                "spark.stop()\n",
                "print(\"✅ Spark session stopped\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}