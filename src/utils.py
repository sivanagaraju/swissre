import logging
from logging.handlers import TimedRotatingFileHandler
import os

import shutil
import glob

def setup_logger(name="ClaimsETL"):
    """
    Sets up a logger with a daily rotating file handler and a stream handler.
    Logs are saved in the 'logs' directory at the project root.
    """
    # Determine the project root (assuming this file is in src/)
    base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    log_dir = os.path.join(base_dir, "logs")
    
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        
    # Generate log file name with current date
    from datetime import datetime
    current_date = datetime.now().strftime("%Y-%m-%d")
    log_file = os.path.join(log_dir, f"etl_job_{current_date}.log")
    
    logger = logging.getLogger(name)
    logger.setLevel(logging.INFO)
    
    # Check if handlers are already added to avoid duplicate logs
    if not logger.handlers:
        # Create formatters
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        # File Handler (New file per day based on filename)
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.INFO)
        
        # Console Handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        console_handler.setLevel(logging.INFO)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
    return logger


def load_config(config_path="config/spark-defaults.conf"):
    """Loads Spark configuration from a file using configparser."""
    # Assuming the config file is relative to the project root or provided absolute path
    if not os.path.exists(config_path):
         # Try finding it relative to this file if not found
         base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
         config_path = os.path.join(base_dir, config_path)
    
    spark_conf = {}
    if os.path.exists(config_path):
        try:
            import configparser
            config = configparser.ConfigParser()
            config.optionxform = str  # Preserve case sensitivity
            
            with open(config_path, 'r') as f:
                # Add a dummy section header to satisfy ConfigParser
                config_string = '[dummy_section]\n' + f.read()
            
            config.read_string(config_string)
            spark_conf = dict(config['dummy_section'])
        except Exception as e:
            logging.getLogger("ClaimsETL").error(f"Failed to load config file: {e}")
            
    return spark_conf


def rename_spark_output(output_dir, destination_path):
    """
    Renames the single CSV part file generated by Spark to a specific destination filename.
    """   
    logger = logging.getLogger("ClaimsETL")
    
    # Find the part file (Spark outputs part-*.csv)
    csv_files = glob.glob(os.path.join(output_dir, "part-*.csv"))
    if csv_files:
        source_file = csv_files[0]
        
        # Move and rename
        shutil.move(source_file, destination_path)
        logger.info(f"Renamed output to {destination_path}")
        return True
    else:
        logger.warning("No output CSV file found to rename.")
        return False

