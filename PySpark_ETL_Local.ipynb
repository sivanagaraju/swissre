{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PySpark ETL with UDF - Local Windows Version\n",
                "\n",
                "**Environment:** Local Windows with PySpark\n",
                "\n",
                "**Approach:** Simple UDF with workarounds for Windows compatibility"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup: Initialize Spark Session\n",
                "\n",
                "Configure Spark for local Windows environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Spark Session Created\n",
                        "Spark Version: 3.5.1\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import sys\n",
                "\n",
                "# Set Python executable for PySpark\n",
                "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
                "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
                "\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# Create Spark session with Windows-friendly config\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"LocalETL\") \\\n",
                "    .master(\"local[*]\") \\\n",
                "    .config(\"spark.driver.memory\", \"2g\") \\\n",
                "    .config(\"spark.sql.shuffle.partitions\", \"2\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"‚úÖ Spark Session Created\")\n",
                "print(f\"Spark Version: {spark.version}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Create Sample Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import Row\n",
                "from datetime import date\n",
                "\n",
                "# Claims data\n",
                "claims_data = [\n",
                "    Row(claim_id=\"CL_001\", policyholder_id=\"PH1\", claim_amount=5000, claim_date=date(2024, 1, 15), region=\"North\"),\n",
                "    Row(claim_id=\"CL_002\", policyholder_id=\"PH2\", claim_amount=3000, claim_date=date(2024, 2, 20), region=\"South\"),\n",
                "    Row(claim_id=\"RX_001\", policyholder_id=\"PH3\", claim_amount=7000, claim_date=date(2024, 3, 10), region=\"East\"),\n",
                "    Row(claim_id=\"CL_003\", policyholder_id=\"PH1\", claim_amount=2000, claim_date=date(2024, 4, 5), region=\"West\"),\n",
                "    Row(claim_id=\"RX_002\", policyholder_id=\"PH4\", claim_amount=4500, claim_date=date(2024, 5, 12), region=\"North\"),\n",
                "    Row(claim_id=\"CL_004\", policyholder_id=\"PH2\", claim_amount=6000, claim_date=date(2024, 6, 18), region=\"South\"),\n",
                "]\n",
                "\n",
                "# Policyholders data\n",
                "policyholders_data = [\n",
                "    Row(policyholder_id=\"PH1\", policyholder_name=\"Alice Johnson\"),\n",
                "    Row(policyholder_id=\"PH2\", policyholder_name=\"Bob Smith\"),\n",
                "    Row(policyholder_id=\"PH3\", policyholder_name=\"Charlie Brown\"),\n",
                "    Row(policyholder_id=\"PH4\", policyholder_name=\"Diana Prince\"),\n",
                "]\n",
                "\n",
                "# Create DataFrames\n",
                "claims_df = spark.createDataFrame(claims_data)\n",
                "policyholders_df = spark.createDataFrame(policyholders_data)\n",
                "\n",
                "print(\"‚úÖ Sample data created\")\n",
                "claims_df.show()\n",
                "policyholders_df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define and Register UDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from pyspark.sql.functions import udf\n",
                "from pyspark.sql.types import StringType\n",
                "\n",
                "# Define the hash function\n",
                "def get_hash(claim_id):\n",
                "    \"\"\"\n",
                "    Fetches MD4 hash for a claim_id from external API.\n",
                "    \"\"\"\n",
                "    if not claim_id:\n",
                "        return \"\"\n",
                "        \n",
                "    try:\n",
                "        url = f\"https://api.hashify.net/hash/md4/hex?value={claim_id}\"\n",
                "        response = requests.get(url, timeout=5)\n",
                "        if response.status_code == 200:\n",
                "            return response.json().get(\"Digest\", \"\")\n",
                "        return \"\"\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        return \"\"\n",
                "\n",
                "# Register as UDF\n",
                "get_hash_udf = udf(get_hash, StringType())\n",
                "\n",
                "print(\"‚úÖ UDF registered\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Extract and Transform Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, when, split, date_format\n",
                "\n",
                "print(\"üîÑ Starting ETL transformation...\\n\")\n",
                "\n",
                "# Step 1: Join Claims and Policyholders\n",
                "print(\"üìä Joining claims and policyholders...\")\n",
                "joined_df = claims_df.join(\n",
                "    policyholders_df, \n",
                "    \"policyholder_id\", \n",
                "    \"left\"\n",
                ")\n",
                "\n",
                "# Step 2: Apply UDF to add hash_id column\n",
                "print(\"üì° Fetching hashes using UDF...\")\n",
                "joined_with_hashes_df = joined_df.withColumn(\"hash_id\", get_hash_udf(col(\"claim_id\")))\n",
                "\n",
                "print(\"\\nüìã DataFrame with Hashes:\")\n",
                "joined_with_hashes_df.select(\"claim_id\", \"policyholder_name\", \"hash_id\").show(truncate=False)\n",
                "\n",
                "# Step 3: Apply business transformations\n",
                "print(\"\\nüîß Applying business transformations...\")\n",
                "final_df = joined_with_hashes_df.withColumn(\n",
                "    \"claim_type\",\n",
                "    when(col(\"claim_id\").like(\"CL%\"), \"Coinsurance\")\n",
                "    .when(col(\"claim_id\").like(\"RX%\"), \"Reinsurance\")\n",
                "    .otherwise(\"Unknown\")\n",
                ").withColumn(\n",
                "    \"claim_priority\",\n",
                "    when(col(\"claim_amount\") > 4000, \"Urgent\")\n",
                "    .otherwise(\"Normal\")\n",
                ").withColumn(\n",
                "    \"claim_period\",\n",
                "    date_format(col(\"claim_date\"), \"yyyy-MM\")\n",
                ").withColumn(\n",
                "    \"source_system_id\",\n",
                "    split(col(\"claim_id\"), \"_\").getItem(1)\n",
                ")\n",
                "\n",
                "# Select final columns in specific order\n",
                "final_df = final_df.select(\n",
                "    \"claim_id\",\n",
                "    \"policyholder_name\",\n",
                "    \"region\",\n",
                "    \"claim_type\",\n",
                "    \"claim_priority\",\n",
                "    \"claim_amount\",\n",
                "    \"claim_period\",\n",
                "    \"source_system_id\",\n",
                "    \"hash_id\"\n",
                ")\n",
                "\n",
                "print(\"\\nüìä Final Transformed DataFrame:\")\n",
                "final_df.show(truncate=False)\n",
                "\n",
                "print(\"\\n‚úÖ Transformation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load - Save Results (Windows-Friendly Approach)\n",
                "\n",
                "Using pandas to avoid Windows worker crashes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "# Define output path (local Windows path)\n",
                "output_file = r\"C:\\Users\\sivan\\Learning\\Code\\swissre\\processed_claims_local.csv\"\n",
                "\n",
                "print(f\"üíæ Saving to {output_file}...\")\n",
                "\n",
                "# Collect to driver and convert to pandas\n",
                "pandas_df = final_df.toPandas()\n",
                "\n",
                "# Write using pandas (more reliable on Windows)\n",
                "pandas_df.to_csv(output_file, index=False)\n",
                "\n",
                "print(\"‚úÖ File saved successfully!\")\n",
                "\n",
                "# Verify\n",
                "print(\"\\nüîç Verifying saved file...\")\n",
                "verified_df = pd.read_csv(output_file)\n",
                "print(f\"‚úÖ Read {len(verified_df)} rows from saved file\")\n",
                "print(verified_df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Alternative: Display Results Without Saving\n",
                "\n",
                "If you just want to see results without file I/O"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show all results\n",
                "print(\"üìä Complete Results:\")\n",
                "final_df.show(100, truncate=False)\n",
                "\n",
                "# Or convert to pandas for better display in Jupyter\n",
                "display(final_df.toPandas())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Cleanup: Stop Spark Session"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stop Spark when done\n",
                "spark.stop()\n",
                "print(\"‚úÖ Spark session stopped\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### ‚úÖ What We Did\n",
                "\n",
                "1. **Created Spark session** with local Windows configuration\n",
                "2. **Defined simple UDF** for hash fetching\n",
                "3. **Applied transformations** using standard PySpark operations\n",
                "4. **Saved results** using pandas (to avoid Windows worker crashes)\n",
                "\n",
                "### ü™ü Windows-Specific Workarounds\n",
                "\n",
                "- ‚úÖ Set `PYSPARK_PYTHON` environment variable\n",
                "- ‚úÖ Use `toPandas()` + pandas `to_csv()` for saving\n",
                "- ‚úÖ Reduced shuffle partitions for small dataset\n",
                "- ‚úÖ Local file paths with `r\"C:\\...\"` format\n",
                "\n",
                "### üöÄ Running This Notebook\n",
                "\n",
                "**Prerequisites:**\n",
                "```bash\n",
                "pip install pyspark pandas requests jupyter\n",
                "```\n",
                "\n",
                "**Run:**\n",
                "```bash\n",
                "jupyter notebook\n",
                "```\n",
                "\n",
                "Then open this notebook and run all cells!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
