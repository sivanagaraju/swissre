{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# PySpark ETL with Simple UDF Hash Fetching\n",
                "\n",
                "**Approach:** Simple UDF - define function, register as UDF, use with `withColumn`\n",
                "\n",
                "Clean and straightforward!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Create Sample Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'spark' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     15\u001b[39m policyholders_data = [\n\u001b[32m     16\u001b[39m     Row(policyholder_id=\u001b[33m\"\u001b[39m\u001b[33mPH1\u001b[39m\u001b[33m\"\u001b[39m, policyholder_name=\u001b[33m\"\u001b[39m\u001b[33mAlice Johnson\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     17\u001b[39m     Row(policyholder_id=\u001b[33m\"\u001b[39m\u001b[33mPH2\u001b[39m\u001b[33m\"\u001b[39m, policyholder_name=\u001b[33m\"\u001b[39m\u001b[33mBob Smith\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     18\u001b[39m     Row(policyholder_id=\u001b[33m\"\u001b[39m\u001b[33mPH3\u001b[39m\u001b[33m\"\u001b[39m, policyholder_name=\u001b[33m\"\u001b[39m\u001b[33mCharlie Brown\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     19\u001b[39m     Row(policyholder_id=\u001b[33m\"\u001b[39m\u001b[33mPH4\u001b[39m\u001b[33m\"\u001b[39m, policyholder_name=\u001b[33m\"\u001b[39m\u001b[33mDiana Prince\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     20\u001b[39m ]\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Create DataFrames\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m claims_df = \u001b[43mspark\u001b[49m.createDataFrame(claims_data)\n\u001b[32m     24\u001b[39m policyholders_df = spark.createDataFrame(policyholders_data)\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Sample data created\u001b[39m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
                    ]
                }
            ],
            "source": [
                "from pyspark.sql import Row\n",
                "from datetime import date\n",
                "\n",
                "# Claims data\n",
                "claims_data = [\n",
                "    Row(claim_id=\"CL_001\", policyholder_id=\"PH1\", claim_amount=5000, claim_date=date(2024, 1, 15), region=\"North\"),\n",
                "    Row(claim_id=\"CL_002\", policyholder_id=\"PH2\", claim_amount=3000, claim_date=date(2024, 2, 20), region=\"South\"),\n",
                "    Row(claim_id=\"RX_001\", policyholder_id=\"PH3\", claim_amount=7000, claim_date=date(2024, 3, 10), region=\"East\"),\n",
                "    Row(claim_id=\"CL_003\", policyholder_id=\"PH1\", claim_amount=2000, claim_date=date(2024, 4, 5), region=\"West\"),\n",
                "    Row(claim_id=\"RX_002\", policyholder_id=\"PH4\", claim_amount=4500, claim_date=date(2024, 5, 12), region=\"North\"),\n",
                "    Row(claim_id=\"CL_004\", policyholder_id=\"PH2\", claim_amount=6000, claim_date=date(2024, 6, 18), region=\"South\"),\n",
                "]\n",
                "\n",
                "# Policyholders data\n",
                "policyholders_data = [\n",
                "    Row(policyholder_id=\"PH1\", policyholder_name=\"Alice Johnson\"),\n",
                "    Row(policyholder_id=\"PH2\", policyholder_name=\"Bob Smith\"),\n",
                "    Row(policyholder_id=\"PH3\", policyholder_name=\"Charlie Brown\"),\n",
                "    Row(policyholder_id=\"PH4\", policyholder_name=\"Diana Prince\"),\n",
                "]\n",
                "\n",
                "# Create DataFrames\n",
                "claims_df = spark.createDataFrame(claims_data)\n",
                "policyholders_df = spark.createDataFrame(policyholders_data)\n",
                "\n",
                "print(\"‚úÖ Sample data created\")\n",
                "claims_df.show()\n",
                "policyholders_df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define and Register UDF"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import requests\n",
                "from pyspark.sql.functions import udf\n",
                "from pyspark.sql.types import StringType\n",
                "\n",
                "# Define the hash function\n",
                "def get_hash(claim_id):\n",
                "    \"\"\"\n",
                "    Fetches MD4 hash for a claim_id from external API.\n",
                "    \"\"\"\n",
                "    if not claim_id:\n",
                "        return \"\"\n",
                "        \n",
                "    try:\n",
                "        url = f\"https://api.hashify.net/hash/md4/hex?value={claim_id}\"\n",
                "        response = requests.get(url, timeout=5)\n",
                "        if response.status_code == 200:\n",
                "            return response.json().get(\"Digest\", \"\")\n",
                "        return \"\"\n",
                "    except Exception:\n",
                "        return \"\"\n",
                "\n",
                "# Register as UDF\n",
                "get_hash_udf = udf(get_hash, StringType())\n",
                "\n",
                "print(\"‚úÖ UDF registered\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Extract and Transform Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql.functions import col, when, split, date_format\n",
                "\n",
                "print(\"üîÑ Starting ETL transformation...\\n\")\n",
                "\n",
                "# Step 1: Join Claims and Policyholders\n",
                "print(\"üìä Joining claims and policyholders...\")\n",
                "joined_df = claims_df.join(\n",
                "    policyholders_df, \n",
                "    \"policyholder_id\", \n",
                "    \"left\"\n",
                ")\n",
                "\n",
                "# Step 2: Apply UDF to add hash_id column\n",
                "print(\"üì° Fetching hashes using UDF...\")\n",
                "joined_with_hashes_df = joined_df.withColumn(\"hash_id\", get_hash_udf(col(\"claim_id\")))\n",
                "\n",
                "print(\"\\nüìã DataFrame with Hashes:\")\n",
                "joined_with_hashes_df.select(\"claim_id\", \"policyholder_name\", \"hash_id\").show(truncate=False)\n",
                "\n",
                "# Step 3: Apply business transformations\n",
                "print(\"\\nüîß Applying business transformations...\")\n",
                "final_df = joined_with_hashes_df.withColumn(\n",
                "    \"claim_type\",\n",
                "    when(col(\"claim_id\").like(\"CL%\"), \"Coinsurance\")\n",
                "    .when(col(\"claim_id\").like(\"RX%\"), \"Reinsurance\")\n",
                "    .otherwise(\"Unknown\")\n",
                ").withColumn(\n",
                "    \"claim_priority\",\n",
                "    when(col(\"claim_amount\") > 4000, \"Urgent\")\n",
                "    .otherwise(\"Normal\")\n",
                ").withColumn(\n",
                "    \"claim_period\",\n",
                "    date_format(col(\"claim_date\"), \"yyyy-MM\")\n",
                ").withColumn(\n",
                "    \"source_system_id\",\n",
                "    split(col(\"claim_id\"), \"_\").getItem(1)\n",
                ")\n",
                "\n",
                "# Select final columns in specific order\n",
                "final_df = final_df.select(\n",
                "    \"claim_id\",\n",
                "    \"policyholder_name\",\n",
                "    \"region\",\n",
                "    \"claim_type\",\n",
                "    \"claim_priority\",\n",
                "    \"claim_amount\",\n",
                "    \"claim_period\",\n",
                "    \"source_system_id\",\n",
                "    \"hash_id\"\n",
                ")\n",
                "\n",
                "print(\"\\nüìä Final Transformed DataFrame:\")\n",
                "final_df.show(truncate=False)\n",
                "\n",
                "print(\"\\n‚úÖ Transformation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Load - Write to Parquet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define output path\n",
                "output_path = \"/tmp/processed_claims_output\"\n",
                "\n",
                "print(f\"üíæ Writing output to {output_path}...\")\n",
                "final_df.coalesce(1).write.parquet(output_path, mode=\"overwrite\")\n",
                "print(\"‚úÖ Write complete!\")\n",
                "\n",
                "# Verify output\n",
                "print(\"\\nüîç Verifying output...\")\n",
                "result_df = spark.read.parquet(output_path)\n",
                "print(f\"‚úÖ Successfully read {result_df.count()} rows from output\")\n",
                "result_df.show(truncate=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Summary\n",
                "\n",
                "### ‚úÖ What We Did\n",
                "\n",
                "**Simple 3-step UDF approach:**\n",
                "\n",
                "1. **Define function** - `get_hash(claim_id)`\n",
                "2. **Register as UDF** - `get_hash_udf = udf(get_hash, StringType())`\n",
                "3. **Use with withColumn** - `df.withColumn(\"hash_id\", get_hash_udf(col(\"claim_id\")))`\n",
                "\n",
                "That's it! Clean and simple.\n",
                "\n",
                "### üìä Complete ETL Pipeline\n",
                "\n",
                "1. ‚úÖ Extract - Load claims and policyholders\n",
                "2. ‚úÖ Transform - Join tables, fetch hashes, apply business rules\n",
                "3. ‚úÖ Load - Write to Parquet\n",
                "\n",
                "### üéØ This Works Great For:\n",
                "\n",
                "- Small to medium datasets\n",
                "- APIs that can handle parallel requests\n",
                "- Databricks/proper Spark cluster environment\n",
                "\n",
                "### üí° Note:\n",
                "\n",
                "This approach creates a **new HTTP connection for each row**.\n",
                "\n",
                "For better performance at scale, consider:\n",
                "- Connection pooling with `mapPartitions`\n",
                "- Driver-side batching with deduplication\n",
                "\n",
                "But for getting started and testing - this simple UDF approach is perfect! ‚ú®"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
